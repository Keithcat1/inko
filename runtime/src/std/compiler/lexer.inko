# Lexing of Inko source code into tokens.
#
# The types and methods of this module are not part of the public API at this
# time, meaning they can change at any time.
import std::byte_array::ToByteArray
import std::compiler::source_location::SourceLocation
import std::compiler::token::*
import std::conversion::ToString
import std::fs::path::(ToPath, Path)
import std::loop::(while)
import std::map::Map
import std::option::Option
import std::range::Range

let INTEGER_DIGIT_RANGE = 48..57
let HEX_LOWER_DIGIT_RANGE = 97..102
let HEX_UPPER_DIGIT_RANGE = 65..70
let LOWER_AZ_RANGE = 97..122
let UPPER_AZ_RANGE = 65..90

let NULL = 0
let TAB = 9
let NEWLINE = 10
let CARRIAGE_RETURN = 13
let ESCAPE = 27
let SPACE = 32
let EXCLAMATION = 33
let DOUBLE_QUOTE = 34
let HASH = 35
let PERCENT = 37
let AMPERSAND = 38
let SINGLE_QUOTE = 39
let PAREN_OPEN = 40
let PAREN_CLOSE = 41
let STAR = 42
let PLUS = 43
let COMMA = 44
let MINUS = 45
let DOT = 46
let SLASH = 47
let ZERO = 48
let COLON = 58
let LOWER = 60
let EQUAL = 61
let GREATER = 62
let QUESTION = 63
let AT_SIGN = 64
let UPPER_E = 69
let UPPER_X = 88
let BRACKET_OPEN = 91
let BACKSLASH = 92
let BRACKET_CLOSE = 93
let CARET = 94
let UNDERSCORE = 95
let BACKTICK = 96
let LOWER_E = 101
let LOWER_N = 110
let LOWER_R = 114
let LOWER_T = 116
let LOWER_X = 120
let CURLY_OPEN = 123
let PIPE = 124
let CURLY_CLOSE = 125
let TILDE = 126

# The default state of the lexer. In this state, code is lexer as you'd expect.
let STATE_DEFAULT = 0

# The state used when lexing a template string.
let STATE_TEMPLATE_STRING = 1

# The maximum value in a `ByteArray`.
let MAX_BYTE = 255

# A type used to map bytes used in escape sequences to the bytes that replace
# them.
#
# An `EscapeMap` can only map bytes up to byte 128.
class EscapeMap {
  # A ByteArray that maps bytes to the ones they should be replaced with.
  #
  # Since NULL bytes are valid replacements, the value 255 is used to signal no
  # replacement exists.
  @mapping: ByteArray

  static def new -> Self {
    Self { @mapping = ByteArray.filled(amount: 128, value: MAX_BYTE) }
  }

  def map(byte: Integer, to: Integer) -> Self {
    @mapping[byte] = to
    self
  }

  def get(input: Integer) -> ?Integer {
    let replacement = @mapping[input]

    (replacement < MAX_BYTE)
      .if(true: { Option.some(replacement) }, false: { Option.none })
  }
}

# Detecting Inko keywords using perfect hashing.
#
# The code for this type was originally generated using gperf using the
# following command:
#
#     gperf -m100 keywords.txt
#
# Here `keywords.txt` contains a sorted list of all the keywords of Inko. The
# generated C code was then ported to Inko by hand.
#
# Since this code is based on generated code, changing it may produce unexpected
# results. For example, the various lookup tables in this type should not have
# their order changed.
class Keywords {
  # The associated values for the first and second bytes in a keyword.
  @associated_values: Array!(Integer)

  # All available keywords, as ByteArray instances.
  @word_list: Array!(ByteArray)

  # Input values with a length outside of this `Range` definitely can't be a
  # keyword.
  @word_length: Range!(Integer)

  # The maximum possible hash value. If a hash value is greater, the input is
  # definitely not a keyword.
  @max_hash_value: Integer

  static def new -> Self {
    Self {
      @associated_values = Array.new(
        24, 24, 24, 24, 24, 24, 24, 24, 24, 24,
        24, 24, 24, 24, 24, 24, 24, 24, 24, 24,
        24, 24, 24, 24, 24, 24, 24, 24, 24, 24,
        24, 24, 24, 24, 24, 24, 24, 24, 24, 24,
        24, 24, 24, 24, 24, 24, 24, 24, 24, 24,
        24, 24, 24, 24, 24, 24, 24, 24, 24, 24,
        24, 24, 24, 24, 24, 24, 24, 24, 24, 24,
        24, 24, 24, 24, 24, 24, 24, 24, 24, 24,
        24, 24, 24, 24, 24, 24, 24, 24, 24, 24,
        24, 24, 24, 24, 24, 24, 24,  0, 24, 12,
        13,  0, 16, 24, 12,  3, 24, 24,  6,  6,
        24,  3, 24, 24,  0,  0,  2, 12, 24,  4,
        24,  9, 24, 24, 24, 24, 24, 24, 24, 24,
        24, 24, 24, 24, 24, 24, 24, 24, 24, 24,
        24, 24, 24, 24, 24, 24, 24, 24, 24, 24,
        24, 24, 24, 24, 24, 24, 24, 24, 24, 24,
        24, 24, 24, 24, 24, 24, 24, 24, 24, 24,
        24, 24, 24, 24, 24, 24, 24, 24, 24, 24,
        24, 24, 24, 24, 24, 24, 24, 24, 24, 24,
        24, 24, 24, 24, 24, 24, 24, 24, 24, 24,
        24, 24, 24, 24, 24, 24, 24, 24, 24, 24,
        24, 24, 24, 24, 24, 24, 24, 24, 24, 24,
        24, 24, 24, 24, 24, 24, 24, 24, 24, 24,
        24, 24, 24, 24, 24, 24, 24, 24, 24, 24,
        24, 24, 24, 24, 24, 24, 24, 24, 24, 24,
        24, 24, 24, 24, 24, 24
      ),
      @word_list = Array.new(
        ''.to_byte_array,
        ''.to_byte_array,
        'as'.to_byte_array,
        ''.to_byte_array,
        'self'.to_byte_array,
        'try'.to_byte_array,
        'return'.to_byte_array,
        'trait'.to_byte_array,
        'static'.to_byte_array,
        'let'.to_byte_array,
        'else'.to_byte_array,
        'match'.to_byte_array,
        'lambda'.to_byte_array,
        'impl'.to_byte_array,
        'local'.to_byte_array,
        'import'.to_byte_array,
        'def'.to_byte_array,
        'yield'.to_byte_array,
        'do'.to_byte_array,
        'throw'.to_byte_array,
        'when'.to_byte_array,
        'mut'.to_byte_array,
        'for'.to_byte_array,
        'class'.to_byte_array
      ),
      @word_length = 2..6,
      @max_hash_value = 23
    }
  }

  # Returns `True` if the given `ByteArray` is a valid Inko keyword.
  def keyword?(bytes: ByteArray) -> Boolean {
    let length = bytes.length

    @word_length.cover?(length).if_false { return False }

    let assoc = @associated_values
    let hash = length + assoc[bytes[1]] + assoc[bytes[0]]

    (hash > @max_hash_value).if_true { return False }

    @word_list[hash] == bytes
  }
}

# A `Lexer` is used for turning Inko source code into a sequence of tokens.
# These tokens in turn can be used by a parser to produce an Abstract Syntax
# Tree.
class Lexer {
  # The byte stream to lex.
  @input: ByteArray

  # The file path that produced the byte stream.
  @file: Path

  # The maximum byte position in the input stream.
  @max_position: Integer

  # The current byte position in the input stream.
  @position: Integer

  # The number of opening curly braces that have yet to be closed.
  @curly_braces: Integer

  # The stack of curly brace counts to use for determining when a template
  # string expression should be closed.
  @curly_brace_stack: Array!(Integer)

  # The stack of lexing states.
  @states: Array!(Integer)

  # The current line number.
  @line: Integer

  # The current column number.
  @column: Integer

  # A type used for detecting keywords.
  @keywords: Keywords

  # The escape sequence literals supported by a single quoted string, and their
  # replacement bytes.
  @single_quoted_string_escapes: EscapeMap

  # The escape sequence literals supported by a double quoted string, and their
  # replacement bytes.
  @double_quoted_string_escapes: EscapeMap

  # The escape sequence literals supported by a template string string, and
  # their replacement bytes.
  @template_string_escapes: EscapeMap

  static def new(input: ToByteArray, file: ToPath) -> Self {
    let bytes = input.to_byte_array

    Self {
      @input = bytes,
      @file = file.to_path,
      @max_position = bytes.length,
      @position = 0,
      @curly_braces = 0,
      @curly_brace_stack = Array.new,
      @states = Array.new(STATE_DEFAULT),
      @line = 1,
      @column = 1,
      @keywords = Keywords.new,
      @single_quoted_string_escapes = EscapeMap
        .new
        .map(byte: SINGLE_QUOTE, to: SINGLE_QUOTE)
        .map(byte: BACKSLASH, to: BACKSLASH),
      @double_quoted_string_escapes = EscapeMap
        .new
        .map(byte: DOUBLE_QUOTE, to: DOUBLE_QUOTE)
        .map(byte: SINGLE_QUOTE, to: SINGLE_QUOTE)
        .map(byte: ZERO, to: NULL)
        .map(byte: BACKSLASH, to: BACKSLASH)
        .map(byte: LOWER_E, to: ESCAPE)
        .map(byte: LOWER_N, to: NEWLINE)
        .map(byte: LOWER_R, to: CARRIAGE_RETURN)
        .map(byte: LOWER_T, to: TAB),
      @template_string_escapes = EscapeMap
        .new
        .map(byte: DOUBLE_QUOTE, to: DOUBLE_QUOTE)
        .map(byte: SINGLE_QUOTE, to: SINGLE_QUOTE)
        .map(byte: ZERO, to: NULL)
        .map(byte: BACKSLASH, to: BACKSLASH)
        .map(byte: BACKTICK, to: BACKTICK)
        .map(byte: LOWER_E, to: ESCAPE)
        .map(byte: LOWER_N, to: NEWLINE)
        .map(byte: LOWER_T, to: TAB)
        .map(byte: LOWER_R, to: CARRIAGE_RETURN)
        .map(byte: CURLY_OPEN, to: CURLY_OPEN)
    }
  }

  # Returns `True` if there is input left to process.
  def next? -> Boolean {
    @position < @max_position
  }

  # Returns the next `Token` available, if any.
  def next -> Token {
    consume_whitespace

    let token = match(@states[-1]) {
      STATE_TEMPLATE_STRING -> { next_template_string_token }
      else -> { next_non_whitespace_token }
    }

    # We have to consume trailing whitespace. Not doing so may result in a
    # consumer of the iterator thinking there are tokens left to produce, when
    # this might not be the case.
    consume_whitespace

    token
  }

  def next_non_whitespace_token -> Token {
    match(current_byte) {
      INTEGER_DIGIT_RANGE -> { number }
      AT_SIGN -> { attribute }
      HASH -> { comment }
      CURLY_OPEN -> { curly_open }
      CURLY_CLOSE -> { curly_close }
      PAREN_OPEN -> { single_character_token('paren_open') }
      PAREN_CLOSE -> { single_character_token('paren_close') }
      SINGLE_QUOTE -> {
        string(
          quote: SINGLE_QUOTE,
          replacements: @single_quoted_string_escapes
        )
      }
      DOUBLE_QUOTE -> {
        string(
          quote: DOUBLE_QUOTE,
          replacements: @double_quoted_string_escapes
        )
      }
      COLON -> { colon }
      SLASH -> { operator(type: 'div', assign_type: 'div_assign') }
      PERCENT -> { percent }
      CARET -> { operator(type: 'xor', assign_type: 'xor_assign') }
      AMPERSAND -> { operator(type: 'and', assign_type: 'and_assign') }
      PIPE -> { operator(type: 'or', assign_type: 'or_assign') }
      STAR -> { star }
      MINUS -> { minus }
      PLUS -> { operator(type: 'add', assign_type: 'add_assign') }
      EQUAL -> { assign }
      LOWER -> { lower }
      GREATER -> { greater }
      BRACKET_OPEN -> { single_character_token('bracket_open') }
      BRACKET_CLOSE -> { single_character_token('bracket_close') }
      EXCLAMATION -> { exclamation }
      DOT -> { dot }
      COMMA -> { single_character_token('comma') }
      QUESTION -> { question }
      UNDERSCORE -> { underscore }
      LOWER_AZ_RANGE -> { identifier_or_keyword }
      UPPER_AZ_RANGE -> { constant }
      BACKTICK -> { template_string_open }
      else -> {
        next?.if(
          true: { invalid },
          false: { NullToken.new(current_location) }
        )
      }
    }
  }

  def next_template_string_token -> Token {
    match(current_byte) {
      BACKTICK -> { return template_string_close }
      CURLY_OPEN -> { return template_string_expression_open }
      NULL -> { return NullToken.new(current_location) }
      else -> { return template_string_text }
    }

    next_template_string_token
  }

  def number(skip_after_start = False) -> Token {
    let mut current = current_byte
    let mut next = next_byte

    # When lexing negative numbers (e.g. -10) we want to skip the first
    # character. This also means we need to peek one byte further, instead of
    # just looking at the next one.
    skip_after_start.if_true {
      current = next_byte
      next = peek(offset: 2)
    }

    # "0x" is only valid at the start of an integer.
    (INTEGER_DIGIT_RANGE.start == current)
      .and { (next == LOWER_X).or { next == UPPER_X } }
      .if(
        true: { hexadecimal_integer(skip_after_start) },
        false: { integer_or_float(skip_after_start) }
      )
  }

  def integer_or_float(skip_after_start = False) -> Token {
    let start = @position
    let line = @line
    let mut type = 'integer'

    skip_after_start.if_true {
      @position += 1
    }

    while(
      true: {
        match(let byte = current_byte) {
          INTEGER_DIGIT_RANGE, UNDERSCORE -> { True }
          LOWER_E, UPPER_E when INTEGER_DIGIT_RANGE =~ next_byte -> {
            type = 'float'
            True
          }
          # If there is an exponent followed by "+" or "-", it's only valid if
          # also followed by a digit. This means "10e+" is invalid, and "10e+2"
          # is valid.
          LOWER_E, UPPER_E
          when
            (next_byte == PLUS)
              .or { next_byte == MINUS }
              .and { INTEGER_DIGIT_RANGE =~ peek(offset: 2) }
          -> {
            @position += 1
            type = 'float'
            True
          }
          DOT when INTEGER_DIGIT_RANGE =~ next_byte -> {
            type = 'float'
            True
          }
          else -> { False }
        }
      },
      then: { @position += 1 }
    )

    token(type: type, start: start, line: line)
  }

  def hexadecimal_integer(skip_after_start = False) -> Token {
    let start = @position
    let line = @line

    skip_after_start.if_true {
      @position += 1
    }

    # Advance 2 for "0x"
    @position += 2

    while(
      true: {
        match(current_byte) {
          INTEGER_DIGIT_RANGE, HEX_LOWER_DIGIT_RANGE,
          HEX_UPPER_DIGIT_RANGE, UNDERSCORE -> { True }
          else -> { False }
        }
      },
      then: { @position += 1 }
    )

    token(type: 'integer', start: start, line: line)
  }

  def comment -> Token {
    let column = @column

    advance_one_byte

    let start = @position

    while({ next?.and { newline?.false? } }) { @position += 1 }

    let comment =
      token(type: 'comment', start: start, line: @line, column: column)

    advance_line
    comment
  }

  def attribute -> Token {
    let start = @position
    let line = @line

    # Advance 1 for the @ sign itself.
    @position += 1

    while({ valid_identifier_byte? }) { @position += 1 }
    token(type: 'attribute', start: start, line: line)
  }

  def single_character_token(type: String) -> Token {
    let start = @position

    @position += 1
    token(type: type, start: start, line: @line)
  }

  def two_character_token(type: String) -> Token {
    let start = @position

    @position += 2
    token(type: type, start: start, line: @line)
  }

  def operator(
    type: String,
    assign_type: String,
    skip_after_start = False
  ) -> Token {
    let start = @position

    skip_after_start.if_true {
      @position += 1
    }

    (next_byte == EQUAL).if(
      true: {
        @position += 2
        binary_assign_token(type: assign_type, start: start, line: @line)
      },
      false: {
        @position += 1
        binary_token(type: type, start: start, line: @line)
      }
    )
  }

  def binary_operator(type: String, equality_type: String) -> Token {
    let start = @position
    let token_type = (next_byte == EQUAL).if(
      true: {
        @position += 2
        equality_type
      },
      false: {
        @position += 1
        type
      }
    )

    binary_token(type: token_type, start: start, line: @line)
  }

  def string(quote: Integer, replacements: EscapeMap) -> Token {
    let column = @column
    let line = @line
    let buffer = ByteArray.new
    let line_buffer = ByteArray.new

    # Advance for the opening quote.
    advance_one_byte

    while({ next?.and { current_byte != quote } }) {
      append_string_byte(
        buffer: buffer,
        line_buffer: line_buffer,
        replacements: replacements
      )
    }

    # Advance for the closing quote.
    advance_one_byte

    let location = source_location(line: line, column: column)
    let value =
      string_value(buffer: buffer, line_buffer: line_buffer, start_line: line)

    RegularToken.new(type: 'string', value: value, location: location)
  }

  def append_string_byte(
    buffer: ByteArray,
    line_buffer: ByteArray,
    replacements: EscapeMap
  ) -> Boolean {
    let current = current_byte

    (current == BACKSLASH).if_true {
      replacements.get(next_byte).let do (replace_with) {
        buffer.push(replace_with)
        line_buffer.push(replace_with)

        @position += 2
        @column += 1

        return True
      }
    }

    buffer.push(current)
    line_buffer.push(current)

    newline?.if(
      true: {
        advance_line
        line_buffer.clear
      },
      false: { @position += 1 }
    )

    False
  }

  def string_value(
    buffer: ByteArray,
    line_buffer: ByteArray,
    start_line: Integer
  ) -> String {
    let value = buffer.drain_to_string

    # If a string contains newlines, we only want to increment the column by the
    # length of the last line.
    #
    # To prevent allocating an additional String, we only use the line buffer if
    # needed.
    let advance_column_by = (@line > start_line).if(
      true: { line_buffer.drain_to_string.length },
      false: { value.length }
    )

    @column += advance_column_by

    value
  }

  def curly_open -> Token {
    @curly_braces += 1
    single_character_token('curly_open')
  }

  def curly_close -> Token {
    @curly_braces.positive?.if_true { @curly_braces -= 1 }

    (@curly_brace_stack.get(-1).get_or(-1) == @curly_braces).if(
      true: {
        @curly_brace_stack.pop
        @states.pop
        single_character_token('tstring_expr_close')
      },
      false: { single_character_token('curly_close') }
    )
  }

  def template_string_open -> Token {
    @states.push(STATE_TEMPLATE_STRING)
    single_character_token('tstring_open')
  }

  def template_string_close -> Token {
    @states.pop
    single_character_token('tstring_close')
  }

  def template_string_expression_open -> Token {
    @states.push(STATE_DEFAULT)
    @curly_brace_stack.push(@curly_braces)

    @curly_braces += 1

    single_character_token('tstring_expr_open')
  }

  def template_string_text -> Token {
    let column = @column
    let line = @line
    let buffer = ByteArray.new
    let line_buffer = ByteArray.new

    while(
      true: {
        # Template strings may be wrapped across lines with a backslash, like
        # so:
        #
        #     `foo \
        #       bar`
        #
        # In this case, the backslash and all whitespace that follows it should
        # be skipped. This means the above is equivalent to:
        #
        #     `foo bar`
        (current_byte == BACKSLASH)
          .and { next_is_whitespace? }
          .if_true {
            advance_one_byte
            consume_whitespace
          }

        match(current_byte) {
          NULL, BACKTICK, CURLY_OPEN -> { False }
          else -> { True }
        }
      },
      then: {
        append_string_byte(
          buffer: buffer,
          line_buffer: line_buffer,
          replacements: @template_string_escapes
        )
      }
    )

    let location = source_location(line: line, column: column)
    let value =
      string_value(buffer: buffer, line_buffer: line_buffer, start_line: line)

    RegularToken.new(type: 'tstring_text', value: value, location: location)
  }

  def colon -> Token {
    let start = @position
    let type = (next_byte == COLON).if(
      true: {
        @position += 2
        'colon_colon'
      },
      false: {
        @position += 1
        'colon'
      }
    )

    token(type: type, start: start, @line)
  }

  def percent -> Token {
    operator(type: 'mod', assign_type: 'mod_assign')
  }

  def star -> Token {
    (next_byte == STAR).if_true {
      return operator(
        type: 'pow',
        assign_type: 'pow_assign',
        skip_after_start: True
      )
    }

    operator(type: 'mul', assign_type: 'mul_assign')
  }

  def minus -> Token {
    match(next_byte) {
      INTEGER_DIGIT_RANGE -> { number(skip_after_start: True) }
      GREATER -> { arrow }
      else -> { operator(type: 'sub', assign_type: 'sub_assign') }
    }
  }

  def assign -> Token {
    let start = @position

    match(next_byte) {
      EQUAL -> {
        @position += 2
        binary_token(type: 'equal', start: start, line: @line)
      }
      TILDE -> {
        @position += 2
        binary_token(type: 'match', start: start, line: @line)
      }
      GREATER -> {
        @position += 2
        token(type: 'double_arrow', start: start, line: @line)
      }
      else -> {
        @position += 1
        token(type: 'assign', start: start, line: @line)
      }
    }
  }

  def lower -> Token {
    (next_byte == LOWER).if_true {
      return operator(
        type: 'shift_left',
        assign_type: 'shift_left_assign',
        skip_after_start: True
      )
    }

    binary_operator(type: 'lower', equality_type: 'lower_equal')
  }

  def greater -> Token {
    (next_byte == GREATER).if_true {
      return operator(
        type: 'shift_right',
        assign_type: 'shift_right_assign',
        skip_after_start: True
      )
    }

    binary_operator(type: 'greater', equality_type: 'greater_equal')
  }

  def not_equal -> Token {
    let start = @position

    @position += 2

    binary_token(type: 'not_equal', start: start, line: @line)
  }

  def arrow -> Token {
    let start = @position

    @position += 2

    token(type: 'arrow', start: start, line: @line)
  }

  def exclamation -> Token {
    match(next_byte) {
      EQUAL -> { not_equal }
      PAREN_OPEN -> { two_character_token('type_args_open') }
      EXCLAMATION -> { two_character_token('throws') }
      else -> { single_character_token('exclamation') }
    }
  }

  def dot -> Token {
    let start = @position

    @position += 1

    (current_byte == DOT).and { next_byte == DOT }.if_true {
      @position += 2

      return binary_token(type: 'exclusive_range', start: start, line: @line)
    }

    (current_byte == DOT).if_true {
      @position += 1

      return binary_token(type: 'inclusive_range', start: start, line: @line)
    }

    token(type: 'dot', start: start, line: @line)
  }

  def question -> Token {
    let start = @position

    @position += 1

    (current_byte == QUESTION).if_true {
      @position += 1

      return binary_token(type: 'question_question', start: start, line: @line)
    }

    token(type: 'question', start: start, line: @line)
  }

  def underscore -> Token {
    let start = @position

    while({ current_byte == UNDERSCORE }) { @position += 1 }

    UPPER_AZ_RANGE.cover?(current_byte).if_true {
      return identifier_or_constant(type: 'constant', start: start)
    }

    identifier_or_constant(type: 'identifier', start: start)
  }

  def identifier_or_keyword -> Token {
    let start = @position

    while({ valid_identifier_byte? }) { @position += 1 }

    let bytes = slice_bytes(start: start, stop: @position)
    let string = bytes.to_string
    let location = source_location(@line)

    @column += string.length

    @keywords.keyword?(bytes).if_true {
      return KeywordToken.new(keyword: string, location: location)
    }

    RegularToken.new(type: 'identifier', value: string, location: location)
  }

  def constant -> Token {
    identifier_or_constant(type: 'constant', start: @position)
  }

  def identifier_or_constant(type: String, start: Integer) -> Token {
    while({ valid_identifier_byte? }) { @position += 1 }
    token(type: type, start: start, line: @line)
  }

  def token(
    type: String,
    start: Integer,
    line: Integer,
    column = @column
  ) -> Token {
    let value = slice_string(start: start, stop: @position)
    let location = source_location(line: line, column: column)

    @column += value.length

    RegularToken.new(type: type, value: value, location: location)
  }

  def binary_token(type: String, start: Integer, line: Integer) -> Token {
    let value = slice_string(start: start, stop: @position)
    let location = source_location(line)

    @column += value.length

    BinaryToken.new(type: type, value: value, location: location)
  }

  def binary_assign_token(type: String, start: Integer, line: Integer) -> Token {
    let value = slice_string(start: start, stop: @position)
    let location = source_location(line)

    @column += value.length

    BinaryAssignToken.new(type: type, value: value, location: location)
  }

  def invalid -> Token {
    let value = slice_string(start: @position, stop: @position + 1)
    let location = source_location(@line)

    # When we run into invalid input we want to immediately stop processing any
    # further input.
    @position = @max_position

    InvalidToken.new(value: value, location: location)
  }

  def source_location(line: Integer, column = @column) -> SourceLocation {
    SourceLocation.new(file: @file, line_range: line..@line, column: column)
  }

  def current_location -> SourceLocation {
    source_location(line: @line)
  }

  # Consumes any whitespace, until reaching the first non-whitespace character.
  def consume_whitespace {
    match(current_byte) {
      SPACE, TAB, CARRIAGE_RETURN -> {
        @position += 1
        @column += 1
      }
      NEWLINE -> { advance_line }
      else -> { return }
    }

    consume_whitespace
  }

  def next_is_whitespace? -> Boolean {
    match(next_byte) {
      SPACE, TAB, CARRIAGE_RETURN, NEWLINE -> { True }
      else -> { False }
    }
  }

  def advance_line {
    @position += 1
    @column = 1
    @line += 1
  }

  def advance_one_byte {
    @position += 1
    @column += 1
  }

  def slice_string(start: Integer, stop: Integer) -> String {
    slice_bytes(start: start, stop: stop).drain_to_string
  }

  def slice_bytes(start: Integer, stop: Integer) -> ByteArray {
    @input.slice(start: start, length: stop - start)
  }

  def current_byte -> Integer {
    next?.if(true: { @input[@position] }, false: { 0 })
  }

  def next_byte -> Integer {
    peek(offset: 1)
  }

  def peek(offset: Integer) -> Integer {
    let index = @position + offset

    (index < @max_position).if(true: { @input[index] }, false: { 0 })
  }

  def valid_identifier_byte? -> Boolean {
    match(current_byte) {
      INTEGER_DIGIT_RANGE, LOWER_AZ_RANGE, UPPER_AZ_RANGE -> { True }
      UNDERSCORE, QUESTION -> { True }
      else -> { False }
    }
  }

  def newline? -> Boolean {
    current_byte == NEWLINE
  }
}
